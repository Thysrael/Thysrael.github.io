<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-wall.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-wall.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta name="baidu-site-verification" content="code-jsxllPgZAX">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.loli.net/css?family=Noto Serif SC:300,300italic,400,400italic,700,700italic|Ma Shan Zheng:300,300italic,400,400italic,700,700italic|JetBrains Mono:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"thysrael.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":false,"style":"flat"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="一、感受这次作业的神经网络是比上一次作业复杂的，上一次作业是“多层感知机（MLP）”，这一次的作业是“卷积神经网络（CNN）”。运算从原来的矩阵乘法变成了卷积，网络结构也更加复杂，所以这次的作业是允许使用框架的。 这次作业的难点本质上是对于卷积网络的理解上，TensorFlow 可以提供卷积的计算方式还有后窥修正，所需要书写的只是网络结构。次难点是 TensorFlow 的工作机理，其实一开始做的">
<meta property="og:type" content="article">
<meta property="og:title" content="IEArch-卷积神经网络">
<meta property="og:url" content="https://thysrael.github.io/posts/5478c034/index.html">
<meta property="og:site_name" content="钟鼓楼">
<meta property="og:description" content="一、感受这次作业的神经网络是比上一次作业复杂的，上一次作业是“多层感知机（MLP）”，这一次的作业是“卷积神经网络（CNN）”。运算从原来的矩阵乘法变成了卷积，网络结构也更加复杂，所以这次的作业是允许使用框架的。 这次作业的难点本质上是对于卷积网络的理解上，TensorFlow 可以提供卷积的计算方式还有后窥修正，所需要书写的只是网络结构。次难点是 TensorFlow 的工作机理，其实一开始做的">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://thysrael.github.io/posts/5478c034/image-20221010192543050.png">
<meta property="og:image" content="https://thysrael.github.io/posts/5478c034/image-20221010193554356.png">
<meta property="og:image" content="https://thysrael.github.io/posts/5478c034/image-20221010194040077.png">
<meta property="og:image" content="https://thysrael.github.io/posts/5478c034/image-20221010211912313.png">
<meta property="og:image" content="https://pic2.zhimg.com/80/v2-1fc2cfc9ef32cddae8921c25dec429f1_720w.webp">
<meta property="og:image" content="https://thysrael.github.io/posts/5478c034/微信图片_20221011114201.png">
<meta property="og:image" content="https://thysrael.github.io/posts/5478c034/image-20221011150430768.png">
<meta property="og:image" content="https://thysrael.github.io/posts/5478c034/微信图片_20221011162019.png">
<meta property="og:image" content="https://thysrael.github.io/posts/5478c034/微信图片_20221011162012.png">
<meta property="og:image" content="https://thysrael.github.io/posts/5478c034/微信图片_20221011152736.jpg">
<meta property="og:image" content="https://thysrael.github.io/posts/5478c034/微信图片_20221011152453.png">
<meta property="og:image" content="https://thysrael.github.io/posts/5478c034/微信图片_20221011152426.png">
<meta property="article:published_time" content="2022-10-10T10:58:27.000Z">
<meta property="article:modified_time" content="2025-08-15T12:16:47.774Z">
<meta property="article:author" content="Thysrael">
<meta property="article:tag" content="直观理解">
<meta property="article:tag" content="S5课上">
<meta property="article:tag" content="IEArch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://thysrael.github.io/posts/5478c034/image-20221010192543050.png">

<link rel="canonical" href="https://thysrael.github.io/posts/5478c034/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>IEArch-卷积神经网络 | 钟鼓楼</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>


<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="钟鼓楼" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>
      <a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/thysrael" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">钟鼓楼</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">钟楼瘦，鼓楼胖</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories<span class="badge">31</span></a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags<span class="badge">70</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives<span class="badge">199</span></a>

  </li>
        <li class="menu-item menu-item-resource">

    <a href="/resource/" rel="section"><i class="fa fa-book fa-fw"></i>Resource</a>

  </li>
        <li class="menu-item menu-item-links">

    <a href="/links/" rel="section"><i class="fa fa-link fa-fw"></i>Links</a>

  </li>
        <li class="menu-item menu-item-roam">

    <a href="/obsidian-quartz/" rel="section"><i class="fa fa-sitemap fa-fw"></i>Roam</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://thysrael.github.io/posts/5478c034/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar2.png">
      <meta itemprop="name" content="Thysrael">
      <meta itemprop="description" content="Can you hear me ?">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="钟鼓楼">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          IEArch-卷积神经网络
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-10-10 18:58:27" itemprop="dateCreated datePublished" datetime="2022-10-10T18:58:27+08:00">2022-10-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-08-15 20:16:47" itemprop="dateModified" datetime="2025-08-15T20:16:47+08:00">2025-08-15</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/IEArch/" itemprop="url" rel="index"><span itemprop="name">IEArch</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span id="busuanzi_value_page_pv"></span>
            </span><br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>7k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>6 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="一、感受"><a href="#一、感受" class="headerlink" title="一、感受"></a>一、感受</h2><p>这次作业的神经网络是比上一次作业复杂的，上一次作业是“多层感知机（MLP）”，这一次的作业是“卷积神经网络（CNN）”。运算从原来的矩阵乘法变成了卷积，网络结构也更加复杂，所以这次的作业是允许使用框架的。</p>
<p>这次作业的难点本质上是对于卷积网络的理解上，TensorFlow 可以提供卷积的计算方式还有后窥修正，所需要书写的只是网络结构。次难点是 TensorFlow 的工作机理，其实一开始做的时候如果能对 TensorFlow 有一个好的理解，写起来大概 40 分钟就够了（但是学习上面这俩东西花了整整一天）。</p>
<hr>
<h2 id="二、卷积神经网络"><a href="#二、卷积神经网络" class="headerlink" title="二、卷积神经网络"></a>二、卷积神经网络</h2><h3 id="2-1-卷积层"><a href="#2-1-卷积层" class="headerlink" title="2.1 卷积层"></a>2.1 卷积层</h3><p>卷积运算利用一个输入矩阵和一个卷积核进行运算，卷积核会在输入矩阵上运动，然后每次都进行与输入矩阵的点对点的乘法（数学上还需要旋转卷积核 180 度，但是实现的时候不需要，因为反正也是随机的），大概如下：</p>
<p><img src="/posts/5478c034/image-20221010192543050.png" alt="image-20221010192543050"></p>
<p>关于卷积为啥要比普通的矩阵乘法（线性变化）要更加优异，我的同学说过，卷积会重复的利用输入矩阵中的信息，而且每个信息的贡献是不等的，边缘的贡献会小一些，而且卷积核这种东西本身的权重就很有灵性，而且可能有更好的数学理论支持。</p>
<p>如果再细说，确实感知机模型在数学上是可以完成拟合的，本质类似于用多项式模拟函数。但是这会导致训练数据集的很大，这是因为本来这件事情就是很困难的事情，但是复杂的神经网络会加入一些假设，正是由于这些假设，导致了我们可以用很小的数据集训练出很高准确率的网络。</p>
<p>以卷积神经网络为例，对于普通的感知机，如果想识别人脸，需要“人脸出现在图片中央，人脸出现在图片角落，人脸出现在图片次中心”等一系列数据，但是卷积神经网络就没有这个限制，卷积的计算方法会天然就可以将这些情况考虑进去。</p>
<p>不同的神经网络有不同的特性，对应不同的假设，如</p>
<ul>
<li>RNN: 时间点和时间点是相关的</li>
<li>CNN: 图片相邻的像素是相关的</li>
<li>Encoder-Decoder: 一个图片的信息可以压缩</li>
<li>UNet: 浅层神经网络对图像边界的理解比深层神经网络好</li>
<li>Dropout: 数据噪声没有意义</li>
<li>各种 attention: 图片某些位置要仔细看</li>
<li>各种 context: 图片中全局的信息很重要</li>
</ul>
<p>不过卷积的特性就是，乘法运算的次数会大大增加（一个输入元素被算了不止一遍），所以才需要池化层和硬件加速器。</p>
<h3 id="2-2-池化层"><a href="#2-2-池化层" class="headerlink" title="2.2 池化层"></a>2.2 池化层</h3><p>池化是为了减少卷积输入矩阵的大小而存在的，其基本原理如下：</p>
<p><img src="/posts/5478c034/image-20221010193554356.png" alt="image-20221010193554356"></p>
<p>对于一个输入矩阵，我们用一个视窗在上面移动，然后从每个视窗中用某种方法获得一个值，然后输出成输出矩阵的一个元素，上图采用的方法是在视窗中挑一个最大的元素当输出，所以叫做 max pool，常用的还有 avg pool ，是取平均值的意思。</p>
<p>经过池化处理，矩阵会变小，其功能有：</p>
<ul>
<li>抑制噪声，降低信息冗余</li>
<li>提升模型的尺度不变性、旋转不变形</li>
<li>降低模型计算量</li>
<li>防止过拟合</li>
</ul>
<p>需要强调的是，视窗的移动不一定不覆盖，如下图所示</p>
<p><img src="/posts/5478c034/image-20221010194040077.png" alt="image-20221010194040077"></p>
<h3 id="2-3-其他概念"><a href="#2-3-其他概念" class="headerlink" title="2.3 其他概念"></a>2.3 其他概念</h3><p>那个移动的视窗也被叫做<strong>滤波器（filter）</strong>，每次移动的步幅叫做<strong>步长（stride）</strong>，另外在卷积中，为了保证输入矩阵和输出矩阵的大小不发生变化（或者可控的变化），我们有在输入矩阵边缘<strong>补零（padding）</strong>的操作。正如多层感知机会有一个 b 的值在完成线性映射后，卷积神经网络也是有这样的值的，我们称之为<strong>偏置单元（bias）</strong></p>
<hr>
<h2 id="三、LeNet"><a href="#三、LeNet" class="headerlink" title="三、LeNet"></a>三、LeNet</h2><h3 id="3-1-总论"><a href="#3-1-总论" class="headerlink" title="3.1 总论"></a>3.1 总论</h3><p>LeNet 是一个经典的卷积网络，有神经网络界的 “Hello, world” 之称，所以网上资料还是很多的。可以大量的查阅资料。</p>
<h3 id="3-2-输入"><a href="#3-2-输入" class="headerlink" title="3.2 输入"></a>3.2 输入</h3><p>虽然在经典的这张图中，一张图片的大小事 <code>32 x 32</code> 的，但是实际在 <code>mnist</code> 数据集中，一张图片的大小是 <code>28 x 28</code> ，所以可以采用 padding 补零的方法让后续操作不太改变。</p>
<p><img src="/posts/5478c034/image-20221010211912313.png" alt=""></p>
<p>通过给定的输入处理函数，我们最终会获得 3 个四维的张量，分别是训练集（train），验证集（validation），测试集（test）。其四个维度为 <code>[batch, height, width, channel]</code> ，其中 <code>batch</code> 为批量之意，在这里是图片数，这里我们的训练集的 <code>batch</code> 为 55000， 验证集的 <code>batch</code> 为 5000，测试集的 <code>batch</code> 为 1000    ，<code>height, width</code> 是图片的高和宽，这里均为 28 ，<code>channel</code> 是通道之意，因为是黑白照片，所以 <code>channel = 1</code> 恒成立（如果是 RGB 彩色图片，则 <code>channel = 3</code>）。</p>
<h3 id="3-3-Conv1"><a href="#3-3-Conv1" class="headerlink" title="3.3 Conv1"></a>3.3 Conv1</h3><p>卷积层的搭建需要用到 TensorFlow 中的函数，如下示例</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>conv2d<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span> <span class="token builtin">filter</span><span class="token punctuation">,</span> strides<span class="token punctuation">,</span> padding<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>其中 <code>input</code> 为输入的四维张量，很好理解。</p>
<p><code>filter</code> 是卷积核的参数，为一个 4 维张量（不是 4 元素向量），每一维格式如下 <code>[长，宽，输入通道数，深度]</code> 。输入通道数应当与 input 的通道数相同，这里是 1，深度（输出通道数）自定，我们这里需要 6 张图（和上面一样，所以为 6），最终代码如下</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 5 * 5 采样窗口，6 个卷积核从 1 个平面抽取特征</span>
conv1_w <span class="token operator">=</span> self<span class="token punctuation">.</span>init_weight<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p>上述代码说明了我们生成了 5 x 5 的输出 6 通道的卷积核（本质应该是 6 个二维矩阵作为卷积核）。</p>
<p>因为最后有 6 张图，所以我们需要 6 个偏置单元</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">conv1_b <span class="token operator">=</span> self<span class="token punctuation">.</span>init_bias<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>和感知器一样，我们需要选择一个激活函数，我们选择 <code>relu</code> ，这个的选择后面会有讨论</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">conv2_h <span class="token operator">=</span> self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv2d<span class="token punctuation">(</span>pool1_h<span class="token punctuation">,</span> conv2_w<span class="token punctuation">)</span><span class="token punctuation">,</span> conv2_b<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>其中的 <code>self.conv2d</code> 是这样的</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">conv2d</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> W<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    param x: input tensor of shape [batch, in_height_, in_weight, in_channels]
    param W: filter tensor of shape [filter_height, filter_width, in_channels, out_channels]
    return: 一组卷积核
    """</span>
    <span class="token comment"># strides 是步长的意思，是 strides[1] 代表 x 方向的步长，strides[2] 代表 y 方向的步长</span>
    <span class="token comment"># padding 即补齐方式，SAME 不会造成输出的矩阵变小（卷积的自然情况），valid 会</span>
    <span class="token keyword">return</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>conv2d<span class="token punctuation">(</span>x<span class="token punctuation">,</span> W<span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'SAME'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>关于步长和补齐，有</p>
<p><img src="https://pic2.zhimg.com/80/v2-1fc2cfc9ef32cddae8921c25dec429f1_720w.webp" alt="img"></p>
<p>其中 <code>stride</code> 作为一个四元数组，<code>stride[0], stride[3] = 1</code> 是常态，这里采用 <code>padding = 'SAME'</code> 的操作，就是为了适应图片是 <code>28 x 28</code> 的特殊性。</p>
<p>设输入图像尺寸为 W，卷积核尺寸为 F，步幅为 S，Padding 使用 P，则经过卷积层或池化层之后的图像尺寸为（W-F+2P）/ S + 1</p>
<ul>
<li>如果参数是 <code>SAME</code>，那么计算只与步长有关，直接除以步长 W / S (除不尽，向上取整)</li>
<li>如果参数是 <code>VALID</code>,那么计算公式如上：(W – F + 1)  /  S （结果向上取整）</li>
</ul>
<p>经过这一层，一张图片被转换成了 6 张 <code>28 x 28</code> 的 feature map。</p>
<h3 id="3-4-Pool1"><a href="#3-4-Pool1" class="headerlink" title="3.4 Pool1"></a>3.4 Pool1</h3><p>池化层的标准都比较统一，有代码</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">pool1_h <span class="token operator">=</span> self<span class="token punctuation">.</span>max_pool<span class="token punctuation">(</span>conv1_h<span class="token punctuation">)</span>
<span class="token keyword">def</span> <span class="token function">max_pool</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    返回一组池化核
    """</span>
    <span class="token comment"># ksize [1,x,y,1] x,y 为池化窗口大小</span>
    <span class="token keyword">return</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>max_pool<span class="token punctuation">(</span>x<span class="token punctuation">,</span> ksize<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'SAME'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>可以看到我们用到的池化层都是每 <code>2 x 2</code> 的窗口中取最大值，然后紧密但是无覆盖的遍历的，并且有补齐操作。这就导致经过池化层后，输入的高和宽都会变为原来的 $\frac12$ 。</p>
<p>经过这一层，6 张 <code>28 x 28</code> 的 feature map 变成了 6 张 <code>14 x 14</code> 的 feature map。</p>
<h3 id="3-5-Conv2"><a href="#3-5-Conv2" class="headerlink" title="3.5 Conv2"></a>3.5 Conv2</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 5 * 5 采样窗口，16 个卷积核从 6 个平面抽取特征</span>
conv2_w <span class="token operator">=</span> self<span class="token punctuation">.</span>init_weight<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
conv2_b <span class="token operator">=</span> self<span class="token punctuation">.</span>init_bias<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">16</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
conv2_h <span class="token operator">=</span> self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv2d<span class="token punctuation">(</span>pool1_h<span class="token punctuation">,</span> conv2_w<span class="token punctuation">)</span><span class="token punctuation">,</span> conv2_b<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<p>结构与 Conv1 类似。</p>
<p>经过这一层， 6 张 <code>14 x 14</code> 的 feature map 变成了 16 张 <code>14 x 14</code> 的 feature map。这里再次与官方的图发生了差异，这是因为原有代码这里肯定没有 padding，但是我 padding 了（使用了课程组提供的函数）。</p>
<h3 id="3-6-Pool2"><a href="#3-6-Pool2" class="headerlink" title="3.6 Pool2"></a>3.6 Pool2</h3><p>结构与 Pool1 类似</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># layer 4 construct the pool2</span>
pool2_h <span class="token operator">=</span> self<span class="token punctuation">.</span>max_pool<span class="token punctuation">(</span>conv2_h<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p>经过这一层，16 张 <code>14 x 14</code> 的 feature map 变成了 16 张 <code>7 x 7</code> 的 feature map。</p>
<h3 id="3-7-Flatten"><a href="#3-7-Flatten" class="headerlink" title="3.7 Flatten"></a>3.7 Flatten</h3><p>我们需要在这里加装一个多层感知机，所以第一步是将原来的四维张量（注意，我们讨论的时候只说了后三维，第一维是图片个数，比较朴素）展成二维张量（其实本质是一维，第一维依然是图片个数），如下实例</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 第四步得到的结果拉伸为 1 个一维向量，其长度为 7 ∗ 7 ∗ 16 = 784</span>
pool2_h_flat <span class="token operator">=</span> tf<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>pool2_h<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">7</span> <span class="token operator">*</span> <span class="token number">7</span> <span class="token operator">*</span> <span class="token number">16</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p>其中 <code>reshape</code> 的用法与 <code>numpy</code> 库类似，<code>-1</code> 代表缺省，需要根据其他维度进行计算。</p>
<p>经过这一层，我们得到了 <code>784</code> 的一维输入。</p>
<h3 id="3-8-Fc1"><a href="#3-8-Fc1" class="headerlink" title="3.8 Fc1"></a>3.8 Fc1</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python">fc1_w <span class="token operator">=</span> self<span class="token punctuation">.</span>init_weight<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">7</span> <span class="token operator">*</span> <span class="token number">7</span> <span class="token operator">*</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">120</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
fc1_b <span class="token operator">=</span> self<span class="token punctuation">.</span>init_bias<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">120</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment"># matmul 是矩阵乘法的意思</span>
fc1_h <span class="token operator">=</span> self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>pool2_h_flat<span class="token punctuation">,</span> fc1_w<span class="token punctuation">)</span><span class="token punctuation">,</span> fc1_b<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<p>这里用 TensorFlow 的形式重新搭建了一遍，感觉十分新奇。</p>
<p>经过这层，<code>784</code> 个输入节点转换成了 <code>120</code> 个隐层节点。可以说从这里开始，又与标准的 LeNet 重合了。</p>
<h3 id="3-9-Fc2"><a href="#3-9-Fc2" class="headerlink" title="3.9 Fc2"></a>3.9 Fc2</h3><p>与 Fc1 结构类似</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">fc2_w <span class="token operator">=</span> self<span class="token punctuation">.</span>init_weight<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">120</span><span class="token punctuation">,</span> <span class="token number">84</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
fc2_b <span class="token operator">=</span> self<span class="token punctuation">.</span>init_bias<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">84</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
fc2_h <span class="token operator">=</span> self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>fc1_h<span class="token punctuation">,</span> fc2_w<span class="token punctuation">)</span><span class="token punctuation">,</span> fc2_b<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<p>经过这一层，<code>120</code> 个隐层节点转换成了 <code>84</code> 个隐层节点。</p>
<h3 id="3-9-Fc3"><a href="#3-9-Fc3" class="headerlink" title="3.9 Fc3"></a>3.9 Fc3</h3><p>与 Fc1 结构类似</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># output layer</span>
fc3_w <span class="token operator">=</span> self<span class="token punctuation">.</span>init_weight<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">84</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
fc3_b <span class="token operator">=</span> self<span class="token punctuation">.</span>init_bias<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
fc3_h <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>fc2_h<span class="token punctuation">,</span> fc3_w<span class="token punctuation">)</span> <span class="token operator">+</span> fc3_b<span class="token punctuation">)</span>  <span class="token comment"># 输出层使用 sigmod 作为激活函数输出</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<p>这层的特殊之处在于激活函数换成了 <code>sigmoid</code> ，这个效果要远好于 <code>rellu</code>。</p>
<p>经过这一层，<code>84</code> 个隐层节点变成了 <code>10</code> 个输出节点。</p>
<hr>
<h2 id="四、TensorFlow"><a href="#四、TensorFlow" class="headerlink" title="四、TensorFlow"></a>四、TensorFlow</h2><h3 id="4-1-张量-Tensor"><a href="#4-1-张量-Tensor" class="headerlink" title="4.1 张量 Tensor"></a>4.1 张量 Tensor</h3><p>张量就是多维矩阵的意思，从上面的分析可知，LeNet 用 TensorFlow 实现是很恰当的。</p>
<h3 id="4-2-Session-和-Computational-Graph"><a href="#4-2-Session-和-Computational-Graph" class="headerlink" title="4.2 Session 和 Computational Graph"></a>4.2 Session 和 Computational Graph</h3><p>对于 TensorFlow，我们大量的运算（看似的运算），其实不是在运算，而是在写一个运算结构，比如说我们搭建的神经网络，其实上面写的东西，并没有实际运算，而是搭建成了这样的一个网络</p>
<p><img src="/posts/5478c034/微信图片_20221011114201.png" alt=""></p>
<p>是一个像表达式树的东西，我们需要利用“对话（session）” 来运行这个结构。</p>
<blockquote>
<p>计算图包含<strong>数据</strong>（data，也就是各种Tensor）和<strong>操作</strong>（operation）。一个模型中的每一个数据或者操作都是图中的一个节点，箭头代表了数据流动的方向。上文提到的用Session计算的过程实际上就是数据流经整个图，计算出每一个节点的数值。</p>
</blockquote>
<h3 id="4-3-数据类型"><a href="#4-3-数据类型" class="headerlink" title="4.3 数据类型"></a>4.3 数据类型</h3><h4 id="4-3-1-类型需求"><a href="#4-3-1-类型需求" class="headerlink" title="4.3.1 类型需求"></a>4.3.1 类型需求</h4><p>对于一个模型，大致有这样几种数据类型需求：</p>
<ul>
<li>可更新的参数：包括权重（weights），偏置项（bias）。这些参数将在训练过程中不断更新。</li>
<li>独立于模型存在的数据：数据集中的数据需要“喂给”网络，包括输入数据、输出端的数据（输出的数据会用于反馈）。这个过程就像数学课本中给函数f(x)求值的过程，给x不同的值，求得不同的结果。可以想见，我们需要一个类似“容器”的对象，每次将数据放进去，然后计算整幅计算图。</li>
<li>常量。</li>
<li>操作符。</li>
</ul>
<p>因此，我们有了这样的几种数据：</p>
<h4 id="4-3-2-Variable"><a href="#4-3-2-Variable" class="headerlink" title="4.3.2 Variable"></a>4.3.2 Variable</h4><p>变量”是可以在程序运行过程中被改变的量，因此<strong>模型中的权重、偏置项等均使用此类型</strong>。从代码中可以看到，是这样的</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token decorator annotation punctuation">@staticmethod</span>
<span class="token keyword">def</span> <span class="token function">init_weight</span><span class="token punctuation">(</span>shape<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    Init weight parameter.
    shape 是一个元组，用于描述矩阵的维度
    """</span>
    w <span class="token operator">=</span> tf<span class="token punctuation">.</span>random<span class="token punctuation">.</span>truncated_normal<span class="token punctuation">(</span>shape<span class="token operator">=</span>shape<span class="token punctuation">,</span> mean<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> stddev<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>w<span class="token punctuation">)</span>

<span class="token decorator annotation punctuation">@staticmethod</span>
<span class="token keyword">def</span> <span class="token function">init_bias</span><span class="token punctuation">(</span>shape<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    Init bias parameter.
    偏置单元（bias unit），在 y = wx + b中，b 表示函数在 y 轴上的截距，控制着函数偏离原点的距离
    """</span>
    b <span class="token operator">=</span> tf<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>shape<span class="token punctuation">)</span>
    <span class="token keyword">return</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>b<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>需要注意的是，<code>Variable</code> 有一个初始化的过程，也就是 <code>tf_mnist.py</code> 中的这个代码</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>global_variables_initializer<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<h4 id="4-3-3-Placeholder"><a href="#4-3-3-Placeholder" class="headerlink" title="4.3.3 Placeholder"></a>4.3.3 Placeholder</h4><p>这个就是前面说的类似“容器”的对象，顾名思义，占位符就是先在模型中”占位“，至于这个位置填入的数值，在Session运行的时候再指定。我们的输入是 Placeholder</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">images <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>dtype<span class="token operator">=</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> shape<span class="token operator">=</span><span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'images'</span><span class="token punctuation">)</span>
labels <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>dtype<span class="token operator">=</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> shape<span class="token operator">=</span><span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'labels'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p>对于这种容器，我们需要在运行的时候“投喂”他，也就是这里的代码</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">_<span class="token punctuation">,</span> loss<span class="token punctuation">,</span> loss_summary <span class="token operator">=</span> sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span><span class="token punctuation">[</span>training_operation<span class="token punctuation">,</span> loss_operation<span class="token punctuation">,</span> merge_summary<span class="token punctuation">]</span><span class="token punctuation">,</span>
                                             feed_dict<span class="token operator">=</span><span class="token punctuation">{</span>images<span class="token punctuation">:</span>batch_train_images<span class="token punctuation">,</span>labels<span class="token punctuation">:</span>batch_train_labels<span class="token punctuation">}</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<h4 id="4-3-4-Operator"><a href="#4-3-4-Operator" class="headerlink" title="4.3.4 Operator"></a>4.3.4 Operator</h4><p>这个东西似乎包括的挺多的，一开始我模型的时候，以为没有设置前向传播和后向反馈的机制，但是后来发现不是在网络里实现的，而是在网络外部实现的，代码如下</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># get loss</span>
cross_entropy <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>softmax_cross_entropy_with_logits<span class="token punctuation">(</span>logits<span class="token operator">=</span>out<span class="token punctuation">,</span> labels<span class="token operator">=</span>labels<span class="token punctuation">)</span>
loss_operation <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_mean<span class="token punctuation">(</span>cross_entropy<span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">"loss"</span><span class="token punctuation">)</span>

<span class="token comment"># set up the optimizer and optimize the parameters</span>
optimizer <span class="token operator">=</span> tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>AdamOptimizer<span class="token punctuation">(</span>learning_rate<span class="token operator">=</span>lr<span class="token punctuation">)</span>
training_operation <span class="token operator">=</span> optimizer<span class="token punctuation">.</span>minimize<span class="token punctuation">(</span>loss_operation<span class="token punctuation">)</span>

<span class="token comment"># post-processing, get accuracy</span>
prediction <span class="token operator">=</span> tf<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>out<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'output'</span><span class="token punctuation">)</span>
correct_prediction <span class="token operator">=</span> tf<span class="token punctuation">.</span>equal<span class="token punctuation">(</span>prediction<span class="token punctuation">,</span> tf<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>labels<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
accuracy_operation <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_mean<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>cast<span class="token punctuation">(</span>correct_prediction<span class="token punctuation">,</span> tf<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">"accuracy"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>这些运算符规定了误差的计算方式、优化器的种类，精确度的计算方式，可以说很神奇了。</p>
<hr>
<h2 id="五、实验结果"><a href="#五、实验结果" class="headerlink" title="五、实验结果"></a>五、实验结果</h2><p>因为我的电脑刚装了  <code>vitis</code> ，导致在运行其他程序的时候经常缺失一大堆库，我努力了俩小时，到现在都没有运行起来 TensorBoard。所以可视化的图用的是同学的。</p>
<h3 id="5-1-训练过程"><a href="#5-1-训练过程" class="headerlink" title="5.1 训练过程"></a>5.1 训练过程</h3><p>训练过程截图：</p>
<p><img src="/posts/5478c034/image-20221011150430768.png" alt="image-20221011150430768"></p>
<p>准确率变化：</p>
<p><img src="/posts/5478c034/微信图片_20221011162019.png" alt=""></p>
<p>loss 变化</p>
<p><img src="/posts/5478c034/微信图片_20221011162012.png" alt=""></p>
<h3 id="5-2-实验结果"><a href="#5-2-实验结果" class="headerlink" title="5.2 实验结果"></a>5.2 实验结果</h3><p>2 号实验结果截图：</p>
<p><img src="/posts/5478c034/微信图片_20221011152736.jpg" alt=""></p>
<p>不同参数数据</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>编号</th>
<th>epoch</th>
<th>激活函数</th>
<th>运行时间(s)</th>
<th>准确率(%)</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>100</td>
<td>最后一层为 tanh，其余为 relu</td>
<td>486.825</td>
<td>98.430</td>
</tr>
<tr>
<td>2</td>
<td>100</td>
<td>最后一层为 sigmoid，其余为 relu</td>
<td>522.454</td>
<td>97.750</td>
</tr>
<tr>
<td>3</td>
<td>30</td>
<td>最后一层为 sigmoid，其余为 relu</td>
<td>146.511</td>
<td>96.510</td>
</tr>
<tr>
<td>4</td>
<td>30</td>
<td>均为 relu</td>
<td>151.085</td>
<td>69.490</td>
</tr>
</tbody>
</table>
</div>
<h3 id="5-3-激活函数的选择"><a href="#5-3-激活函数的选择" class="headerlink" title="5.3 激活函数的选择"></a>5.3 激活函数的选择</h3><p>之所以讨论这个问题，是因为我的同学敲代码的时候将最后一层本来的 <code>sigmoid</code> 敲成 <code>relu</code>，然后导致她的正确率一直是 40% 多，最后我俩一起 de 了这个 bug，然后对激活函数的影响有了深刻认识。</p>
<p><img src="/posts/5478c034/微信图片_20221011152453.png" alt=""></p>
<p>总的来说，不同的激活函数对与学习速度，过拟合现象，死神经现象都有影响，导致最终对于正确率也有一定的影响，比较详细的是和激活函数的梯度有关系，当然也和均值啥的有关系，我找到一篇写得很好的帖子，附在这里 <a target="_blank" rel="noopener external nofollow noreferrer" href="https://zhuanlan.zhihu.com/p/427541517?utm_campaign=shareopn&amp;utm_medium=social&amp;utm_oi=1124048358603636736&amp;utm_psn=1563147188217319424&amp;utm_source=wechat_session">详解激活函数</a> 。另外并不是准确率越高越好，还要考虑训练时间、训练规模等因素。</p>
<p>在这个数据集和网络下，激活函数可以绘图如下</p>
<p><img src="/posts/5478c034/微信图片_20221011152426.png" alt=""></p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3/" rel="tag"><i class="fa fa-tag"></i> 直观理解</a>
              <a href="/tags/S5%E8%AF%BE%E4%B8%8A/" rel="tag"><i class="fa fa-tag"></i> S5课上</a>
              <a href="/tags/IEArch/" rel="tag"><i class="fa fa-tag"></i> IEArch</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/posts/de998002/" rel="prev" title="补充数学-单纯形法">
      <i class="fa fa-chevron-left"></i> 补充数学-单纯形法
    </a></div>
      <div class="post-nav-item">
    <a href="/posts/d9d8ff0a/" rel="next" title="数学建模-Python相关库">
      数学建模-Python相关库 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
<script src="https://utteranc.es/client.js"
        repo="Thysrael/blog-comment"
        issue-term="title"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E3%80%81%E6%84%9F%E5%8F%97"><span class="nav-text">一、感受</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E3%80%81%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-text">二、卷积神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-%E5%8D%B7%E7%A7%AF%E5%B1%82"><span class="nav-text">2.1 卷积层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-%E6%B1%A0%E5%8C%96%E5%B1%82"><span class="nav-text">2.2 池化层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-%E5%85%B6%E4%BB%96%E6%A6%82%E5%BF%B5"><span class="nav-text">2.3 其他概念</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%89%E3%80%81LeNet"><span class="nav-text">三、LeNet</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-%E6%80%BB%E8%AE%BA"><span class="nav-text">3.1 总论</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-%E8%BE%93%E5%85%A5"><span class="nav-text">3.2 输入</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-Conv1"><span class="nav-text">3.3 Conv1</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-Pool1"><span class="nav-text">3.4 Pool1</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-5-Conv2"><span class="nav-text">3.5 Conv2</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-6-Pool2"><span class="nav-text">3.6 Pool2</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-7-Flatten"><span class="nav-text">3.7 Flatten</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-8-Fc1"><span class="nav-text">3.8 Fc1</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-9-Fc2"><span class="nav-text">3.9 Fc2</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-9-Fc3"><span class="nav-text">3.9 Fc3</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%9B%E3%80%81TensorFlow"><span class="nav-text">四、TensorFlow</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-%E5%BC%A0%E9%87%8F-Tensor"><span class="nav-text">4.1 张量 Tensor</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-Session-%E5%92%8C-Computational-Graph"><span class="nav-text">4.2 Session 和 Computational Graph</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B"><span class="nav-text">4.3 数据类型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-3-1-%E7%B1%BB%E5%9E%8B%E9%9C%80%E6%B1%82"><span class="nav-text">4.3.1 类型需求</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-3-2-Variable"><span class="nav-text">4.3.2 Variable</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-3-3-Placeholder"><span class="nav-text">4.3.3 Placeholder</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-3-4-Operator"><span class="nav-text">4.3.4 Operator</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%94%E3%80%81%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C"><span class="nav-text">五、实验结果</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B"><span class="nav-text">5.1 训练过程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C"><span class="nav-text">5.2 实验结果</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-3-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E7%9A%84%E9%80%89%E6%8B%A9"><span class="nav-text">5.3 激活函数的选择</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Thysrael"
      src="/images/avatar2.png">
  <p class="site-author-name" itemprop="name">Thysrael</p>
  <div class="site-description" itemprop="description">Can you hear me ?</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">199</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">31</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">70</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="">
    <a target="_blank" class="social-link" href="/atom.xml" style="color: burlywood;">
      <span class="icon">
        <i class="fa fa-rss"></i>
      </span>
      <span class="label">RSS</span>
    </a>
  </div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/thysrael" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;thysrael" rel="noopener external nofollow noreferrer" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:thysrael@163.com" title="E-Mail → mailto:thysrael@163.com" rel="noopener external nofollow noreferrer" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2021.12.18 – 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Thysrael</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="Symbols count total">1.3m</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="Reading time total">20:17</span>
</div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  
  <script>
    (function(){
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x=document.getElementsByTagName("link");
		//Find the last canonical URL
		if(x.length > 0){
			for (i=0;i<x.length;i++){
				if(x[i].rel.toLowerCase() == 'canonical' && x[i].href){
					canonicalURL=x[i].href;
				}
			}
		}
    //Get protocol
	    if (!canonicalURL){
	    	curProtocol = window.location.protocol.split(':')[0];
	    }
	    else{
	    	curProtocol = canonicalURL.split(':')[0];
	    }
      //Get current URL if the canonical URL does not exist
	    if (!canonicalURL) canonicalURL = window.location.href;
	    //Assign script content. Replace current URL with the canonical URL
      !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=canonicalURL,t=document.referrer;if(!e.test(r)){var n=(String(curProtocol).toLowerCase() === 'https')?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";t?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var i=new Image;i.src=n}}(window);})();
  </script>




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
