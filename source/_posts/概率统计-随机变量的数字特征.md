---
abbrlink: 3529af83
categories: 概率统计
date: 2021-12-19 15:54:42
mathjax: true
tags: [概率统计, S3复习, 直观理解, 知识总结]
title: 概率统计-随机变量的数字特征
---

在前面几章，我说关于随机变量，一个重要的是判断是分布类型，另一个是参数，现在可以将参数改为数字特征了。

之前我还像把分布作为一个更高层次抽象出来理解，但是现在看来随机变量可以完全支配其他概念，应该是我的理解不到位了，随机变量是一个更为宏大普世的概念。

在许多实际问题中，一般并不需要分布函数，只需要知道随机变量的某些特征就够了。如果这个角度来看，随机变量应该不是一个**仅仅服务于概率论**的概念，他应该是在统计学中也有应用。

之前我一直想的都是随机变量是建立在概率空间 $(\Omega, F, P)$ 上的一个分支或者应用，但是如果换一种角度思考，**将随机变量看成一个完整的体系**，然后在利用随机变量的映射本质沟通这两个体系，或许是一种好的理解方法。之前的理解过度关注随机变量的映射功能，而对其函数特性的讨论较为疏忽。

也就是说，将离散变量看成**抽象的数据组**，然后分析它的分布类型和数字特征，最后将某个试验的事件与它进行映射对应。

相关性（线性相关性）的本质是 $P(Y = aX + b) = 1$

<!-- more -->

## 一、数学期望

### 1.1 定义

需要注意的是，不是每一个随机变量都有期望的，期望要求**积分的绝对收敛**（我不知道为什么不是条件收敛）。
$$
EX = \int^{+\infty}_{-\infty}xf(x)dx
$$
还是很自然的一个概念。

如果求 $X$ 的函数 $Y = g(X)$ 的数学期望，有：

$$
EY = \int^{+\infty}_{-\infty}g(x)f(x)dx
$$
对于**随机向量**，应该是没有办法求它的期望的（虽然我不知道为啥不能，感觉从直观上还是可以理解出一个期望概念的），书上只讲了将随机向量先映射成一个随机变量，然后再求这个像的期望的方法，但是其实我觉得就已经足够了，因为这个已经很原子操作了，如果像有一个**期望随机向量**，其实也是每个分量随机标量这样一个一个求解：

设 $(X, Y)$ 为随机变量，$g(x,y)$ 为连续函数，那么 $Z = g(X, Y)$ 的期望是

$$
EZ = Eg(X, Y) = \int^{+\infty}_{-\infty}\int^{+\infty}_{-\infty}g(x, y)f(x, y)dxdy
$$

### 1.2 性质

$$
E(\sum^n_{i = 1} k_iX_i) = \sum^n_{i = 1}k_i EX_i
$$

应该是满足数乘和加法。
$$
E(EX) = EX
$$
这其实是常数的期望是就是常数本身。因为在后面的计算中时常出现期望嵌套期望的现象，意识到这点再结合上面的加法和数乘，可以用于化简。	

如果 $X,Y$ 为互相独立的随机变量，且 $E\mid X \mid, E\mid Y \mid, E\mid XY \mid$ 存在，则
$$
EXY = EX\cdot EY
$$
从后面的种种性质中我们可以看出，当满足**相互独立**的条件的时候，有很多很有用的结论。相互独立，或许应该作为一个研究对象专门进行研究。

此外，好像样本矩的符号表示似乎非常随便，有的有括号，有的没括号，似乎没有统一的定论，所以做题的时候不要慌就好了。

---



## 二、方差

### 2.1 定义

$$
DX = E(X - EX)=EX^2 - (EX)^2
$$

如果硬要理解的话，可以说平方有一个放大差异的过程，期望有一个缩小差异的过程。

### 2.2 性质

若 $C$ 为常数（其实这里和上面都表述的不完善，应该说是常值函数），有 
$$
DC = 0
$$
若 $b,c$ 为常数，$X$ 随机变量，有（这里的b,c才是常数的意思）
$$
D(cX + b) = c^2DX
$$
如果 $X,Y$ 是相互独立的随机变量，有
$$
D(aX + bY + c) = a^2DX + b^2DY
$$

$$
DXY = EX^2\cdot EY^2 - (EX)^2 \cdot (EY)^2
$$

---



## 三、常用随机变量的期望和方差

可以看到，这里就开始强调随机变量的主体地位，以随机变量为主体，分布类型和数字特征都是观测的一个角度。

### 3.1 两点分布

$$
EX = p
$$

$$
DX = p(1-p)
$$

### 3.2 二项分布

$$
EX = np
$$

$$
DX = np(1-p)
$$

### 3.3 泊松分布

$$
EX = \lambda
$$

$$
DX = \lambda
$$

### 3.4 均匀分布

设 $X$ 在区间 $[a, b]$ 上服从均匀分布，有：
$$
EX = \frac{a + b}{2}
$$

$$
DX = \frac{(b - a)^2}{2}
$$

### 3.5 指数分布

$$
EX = \frac{1}{\lambda}
$$

$$
DX = \frac{1}{\lambda ^2}
$$

### 3.6 正态分布

$$
EX = \mu
$$

$$
DX = \sigma^2
$$

---



## 四、相关性

### 4.1 协方差

协方差描述的是两个变量的相关性，有定义
$$
Cov(X, Y) = E((X - EX)\cdot(Y - EY)) = EXY - EX \cdot EY
$$
有如下性质
$$
Cov(X, Y) = Cov(Y,X)
$$

$$
Cov(aX, bY) = abCov(X, Y)
$$

$$
Cov(X_1 + X_2, Y) = Cov(X_1, Y) + Cov(X_2, Y)
$$

$$
D(X + Y) = DX + DY + Cov(X, Y)
$$

$$
D(X - Y) = DX + DY - Cov(X, Y)
$$

### 4.2 协方差矩阵

对于n为随机向量 $(X_1, X_2,\cdots,X_n)$ ，若
$$
C_{ij} = Cov(X_i, X_j), C =(C_{ij}) _{n\times n}
$$
则称 $C$ 为随机向量 $(X_1, X_2,\cdots,X_n)$ 的协方差矩阵。这个启发我，是不是有些数字特征不是限于一个随机变量，只有有多个随机变量的时候，有些数字特征才会显现出来，或者说，数字特征并不局限于标量，随机向量也适用。

### 4.3 相关系数

$$
\rho_{XY} = \frac{Cov(X, Y)}{\sqrt{DX}\sqrt{DY}}
$$

这个既叫做 $X,Y$ 的相关系数，又叫做 $X,Y$ 的标准协方差。相关系数是一个恒小于等于1的数，证明方法可以利用**柯西不等式**进行证明。

### 4.4 相关性和独立性

首先先明确，这个两个概念的关系是很弱的。

相关性其本质是
$$
P(Y = aX + b) = 1
$$
说的是随机变量 $X,Y$ 之间**线性关系**的近似程度，我们假设两者线性相关
$$
Y = aX + b,EY = E(aX + b) = aEX + b
$$
$$
Y - EY = a(X - EX)
$$

也就是说，我们可以通过检测构造 $Y-EY, X - EX$ 随机向量的方法来检测相关性。

那么是不是说 $\rho = 0$ 就一定相关性为 0 了呢？或者为了避免方差为0，协方差为0就一定相关性为0了呢？不是，如果 $X,Y$ 是一个常值随机变量，那么 $EXY - EXEY = 0$，此时 $Cov(X, Y)$ 为0，但是 $P(Y = 0X + EY) = 1$。所以两者的相关性超好。所以说，相关系数只是衡量相关性的一个维度，而不能认为其就是相关性本身。

虽然没有经过严格的数学证明，但是我觉得讲到这个已经可以认为相关性和独立性关系不是那么大了。说不定虽然不满足线性相关性的随机变量，满足其他形式的相关呢。但是我不知道这个角度正不正确。反正最终结论是：**独立性是比相关性更强的条件，由随机变量独立可以推出不相关，但是不相关不能推出独立**。

但是对于特定的某个分布，还是可以把条件放松的，比如说**对于正态分布，独立性和不相关性是充要条件**。

---



## 五、矩

矩是一些数字特征的泛称或者总称。

$E(X^K)$ 是 $X$ 的 k 阶原点矩。$E(X - EX)^k$ 是 $X$ 的 k 阶中心距。

对于正态分布，对于 k 阶中心距，有当 k 为奇数的时候，中心距为0，当 k 为偶数的时候
$$
E(X - EX)^k = \sigma^k k!!
$$
